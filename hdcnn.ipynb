{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchically Deep Convolutional Neural Network For Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as kr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.datasets import cifar100\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2 \n",
    "\n",
    "from random import randint\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # Don't pre-allocate memory; allocate as-needed\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3 # Only allow a total fraction the GPU memory to be allocated\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/models/'):\n",
    "    os.mkdir('data/models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GGN = True\n",
    "\n",
    "# The number of coarse categories\n",
    "coarse_categories = 20\n",
    "\n",
    "# The number of fine categories\n",
    "fine_categories = 61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Cifar100 Data Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X, y_c), (x_test, y_c_test) = cifar100.load_data(label_mode='coarse')\n",
    "# (X, y), (x_test, y_test) = cifar100.load_data(label_mode='fine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Cham Data Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"dataset/train_feature.npy\")\n",
    "y = np.load(\"dataset/train_label.npy\")\n",
    "y_c = np.load(\"dataset/train_label_coarse.npy\")\n",
    "\n",
    "x_test = np.load(\"dataset/test_feature.npy\")\n",
    "y_test = np.load(\"dataset/test_label.npy\")\n",
    "y_c_test = np.load(\"dataset/test_label_coarse.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-To-Coarse Mapping**\n",
    "\n",
    "(Ideally, this would be done through spectral clustering as opposed to hard-coding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine2coarse = np.zeros((fine_categories,coarse_categories))\n",
    "for i in range(coarse_categories):\n",
    "    index = np.where(y_c_test[:,0] == i)[0]\n",
    "    fine_cat = np.unique([y_test[j,0] for j in index])\n",
    "    if len(index) == 0:\n",
    "        print i\n",
    "    for j in fine_cat:\n",
    "        fine2coarse[j,i] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_c = 0; # Clear y_c in interest of saving mem\n",
    "y_c_test=0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: One Hot Encoding\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function extends a matrix to one-hot encoding\n",
    "#    \n",
    "#    Parameters:\n",
    "#        y    Array of label values\n",
    "# \n",
    "#    Returns:\n",
    "#        y_new    One hot encoded array of labels\n",
    "################################################################################\n",
    "def one_hot(y):\n",
    "    n_values = np.max(y) + 1\n",
    "    y_new = np.eye(n_values)[y[:,0]]\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2359, 61)\n"
     ]
    }
   ],
   "source": [
    "y=one_hot(y)\n",
    "y_test=one_hot(y_test)\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply ZCA Whitening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: ZCA\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function applies ZCA Whitening to the image set\n",
    "#    \n",
    "#    Parameters:\n",
    "#        x_1           Array of MxNxC images to compute the ZCA Whitening\n",
    "#        x_2           Array of MxNxC images to apply the ZCA transform\n",
    "#        num_batch    Number of batches to do the computation\n",
    "# \n",
    "#    Returns:\n",
    "#        An array of MxNxC zca whitened images\n",
    "################################################################################\n",
    "def zca(x_1, x_2, epsilon=1e-5):\n",
    "        \n",
    "    with tf.name_scope('ZCA'):\n",
    "        \n",
    "        x1 = tf.placeholder(tf.float64, shape=np.shape(x_1), name='placeholder_x1')\n",
    "        x2 = tf.placeholder(tf.float64, shape=np.shape(x_2), name='placeholder_x2')\n",
    "        \n",
    "        flatx = tf.cast(tf.reshape(x1, (-1, np.prod(x_1.shape[-3:])),name=\"reshape_flat\"),tf.float64,name=\"flatx\")\n",
    "        sigma = tf.tensordot(tf.transpose(flatx),flatx, 1,name=\"sigma\") / tf.cast(tf.shape(flatx)[0],tf.float64) ### N-1 or N?\n",
    "        s, u, v = tf.svd(sigma,name=\"svd\")\n",
    "        pc = tf.tensordot(tf.tensordot(u,tf.diag(1. / tf.sqrt(s+epsilon)),1,name=\"inner_dot\"),tf.transpose(u),1, name=\"pc\")\n",
    "        \n",
    "        net1 = tf.tensordot(flatx, pc,1,name=\"whiten1\")\n",
    "        net1 = tf.reshape(net1,np.shape(x_1), name=\"output1\")\n",
    "        \n",
    "        flatx2 = tf.cast(tf.reshape(x2, (-1, np.prod(x_2.shape[-3:])),name=\"reshape_flat2\"),tf.float64,name=\"flatx2\")\n",
    "        net2 = tf.tensordot(flatx2, pc,1,name=\"whiten2\")\n",
    "        net2 = tf.reshape(net2,np.shape(x_2), name=\"output2\")\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            x_1,x_2 = sess.run([net1,net2], feed_dict={x1: x_1, x2: x_2})    \n",
    "    return x_1,x_2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "time1 = time.time()\n",
    "X,x_test = zca(X,x_test)\n",
    "time2 = time.time()\n",
    "print('Time Elapsed - ZCA Whitening: '+str(time2-time1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Training set into Training and Validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=.1, stratify=y, random_state=0)\n",
    "X = 0\n",
    "y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_cifar100_data(X_train, Y_train, X_valid, Y_valid, img_rows, img_cols):\n",
    "    \n",
    "    nb_train_samples = 3000 # 3000 training samples\n",
    "    nb_valid_samples = 100 # 100 validation samples\n",
    "    # Load cifar10 training and validation sets\n",
    "    # (X_train, Y_train), (X_valid, Y_valid) = cifar10.load_data()\n",
    "\n",
    "    # Resize trainging images\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        X_train = np.array([cv2.resize(img.transpose(1,2,0), (img_rows,img_cols)).transpose(2,0,1) for img in X_train[:nb_train_samples,:,:,:]])\n",
    "        X_valid = np.array([cv2.resize(img.transpose(1,2,0), (img_rows,img_cols)).transpose(2,0,1) for img in X_valid[:nb_valid_samples,:,:,:]])\n",
    "    else:\n",
    "        X_train = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_train[:nb_train_samples,:,:,:]])\n",
    "        X_valid = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_valid[:nb_valid_samples,:,:,:]])\n",
    "\n",
    "    # Transform targets to keras compatible format\n",
    "#     Y_train = np_utils.to_categorical(Y_train[:], num_classes)\n",
    "#     Y_valid = np_utils.to_categorical(Y_valid[:], num_classes)\n",
    "    Y_train = Y_train[:nb_train_samples]\n",
    "    Y_valid = Y_valid[:nb_valid_samples]\n",
    "    \n",
    "    return X_train, Y_train, X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# only for CIFAR100\n",
    "if GGN:\n",
    "    x_train, y_train, x_val, y_val = resize_cifar100_data(x_train, y_train, x_val, y_val, 224, 224)\n",
    "else:\n",
    "    x_train, y_train, x_val, y_val = resize_cifar100_data(x_train, y_train, x_val, y_val, 32, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flip, pad and randomly crop each photo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Preprocess Img\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function pads images by 4 pixels, randomly crops them, then\n",
    "#        randomly flips them\n",
    "#    \n",
    "#    Parameters:\n",
    "#        x_1           Array of MxNxC images to compute the ZCA Whitening\n",
    "#        x_2           Array of MxNxC images to apply the ZCA transform\n",
    "#        num_batch    Number of batches to do the computation\n",
    "# \n",
    "#    Returns:\n",
    "#        An array of MxNxC zca whitened images\n",
    "################################################################################\n",
    "def preprocess_img(X,y):\n",
    "        \n",
    "    with tf.name_scope('Preproc'):\n",
    "        \n",
    "        images = tf.placeholder(tf.float64, shape=np.shape(X))\n",
    "        labels = tf.placeholder(tf.float64, shape=np.shape(y))\n",
    "        \n",
    "        net = tf.map_fn(lambda img: tf.image.flip_left_right(img), images)\n",
    "        net = tf.map_fn(lambda img: tf.image.rot90(img), net)\n",
    "        net = tf.image.resize_image_with_crop_or_pad(net,40,40)\n",
    "        net = tf.map_fn(lambda img: tf.random_crop(img, [32,32,3]), net)\n",
    "\n",
    "        net1 = tf.image.resize_image_with_crop_or_pad(images,40,40)\n",
    "        net1 = tf.map_fn(lambda img: tf.random_crop(img, [32,32,3]), net1)\n",
    "        \n",
    "        net = tf.concat([net, net1],0)\n",
    "        net = tf.random_shuffle(net, seed=0)\n",
    "        net_labels = tf.concat([labels, labels],0)\n",
    "        net_labels = tf.random_shuffle(net_labels,seed=0)\n",
    "        \n",
    "        net = tf.map_fn(lambda img: tf.image.random_flip_up_down(img), net)\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            x_t,y_t = sess.run([net,net_labels], feed_dict={images: X, labels: y})    \n",
    "    return x_t,y_t\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "time1 = time.time()\n",
    "x_train,y_train = preprocess_img(x_train,y_train)\n",
    "time2 = time.time()\n",
    "print('Time Elapsed - Img Preprocessing: '+str(time2-time1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.layers import Input, Conv2D, Dropout, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# in_layer = Input(shape=(32, 32, 3), dtype='float32', name='main_input')\n",
    "\n",
    "# net = Conv2D(384, 3, strides=1, padding='same', activation='elu')(in_layer)\n",
    "# net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "\n",
    "# net = Conv2D(384, 1, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Conv2D(384, 2, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Conv2D(640, 2, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Conv2D(640, 2, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Dropout(.2)(net)\n",
    "# net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "\n",
    "# net = Conv2D(640, 1, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Conv2D(768, 2, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Conv2D(768, 2, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Conv2D(768, 2, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Dropout(.3)(net)\n",
    "# net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "\n",
    "# net = Conv2D(768, 1, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Conv2D(896, 2, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Conv2D(896, 2, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Dropout(.4)(net)\n",
    "# net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "\n",
    "# net = Conv2D(896, 3, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Conv2D(1024, 2, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Conv2D(1024, 2, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Dropout(.5)(net)\n",
    "# net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "\n",
    "# net = Conv2D(1024, 1, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Conv2D(1152, 2, strides=1, padding='same', activation='elu')(net)\n",
    "# net = Dropout(.6)(net)\n",
    "# net = MaxPooling2D((2, 2), padding='same')(net)\n",
    "\n",
    "# net = Flatten()(net)\n",
    "# net = Dense(1152, activation='elu')(net)\n",
    "# net = Dense(100, activation='softmax')(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision_m = precision(y_true, y_pred)\n",
    "    recall_m = recall(y_true, y_pred)\n",
    "    return 2*((precision_m*recall_m)/(precision_m+recall_m+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GGN: \n",
    "    # Inception v1\n",
    "    from inception_v1 import InceptionV1\n",
    "    in_layer, net, model = InceptionV1(weights=None,classes=fine_categories) # classes = sum of all fine classes\n",
    "else:\n",
    "    # NiN\n",
    "    model = Model(inputs=in_layer,outputs=net)\n",
    "    \n",
    "sgd_coarse = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer= sgd_coarse, loss='categorical_crossentropy', metrics=['accuracy', f1_score, precision, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('data/models/model_coarse'+str(30)+'.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training history visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_histo_vis(history,title):\n",
    "        \n",
    "    acc = [hist.history['f1_score'] for hist in history]\n",
    "    val_acc = [hist.history['val_f1_score'] for hist in history]\n",
    "    loss = [hist.history['loss'] for hist in history]\n",
    "    val_loss = [hist.history['val_loss'] for hist in history]\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(epochs, acc, 'b', label='Training F1_score')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation F1_score')\n",
    "    plt.title('Training and validation F1_score')\n",
    "    plt.ylabel('F1_score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(title+\"_f1_score.png\",bbox_inches='tight')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(title+\"_loss.png\",bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_his(history_model, history_path):\n",
    "    # Get the dictionary containing each metric and the loss for each epoch\n",
    "    history_dict = {}\n",
    "    for hist in history_model:\n",
    "        history_dict.update(hist.history)\n",
    "    # Save it under the form of a json file\n",
    "    json.dump(history_dict, open(history_path, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbCallBack = kr.callbacks.TensorBoard(log_dir='./data/graph/elu_drop/', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Train on 2123 samples, validate on 236 samples\n",
      "Epoch 1/1000\n",
      "2123/2123 [==============================] - 20s 10ms/step - loss: 3.6161 - acc: 0.0989 - f1_score: 0.0179 - precision: 0.0710 - recall: 0.0104 - val_loss: 4.7143 - val_acc: 0.1144 - val_f1_score: 0.1158 - val_precision: 0.1933 - val_recall: 0.0890\n",
      "Epoch 2/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 2.8364 - acc: 0.2407 - f1_score: 0.1331 - precision: 0.3843 - recall: 0.0838 - val_loss: 5.9041 - val_acc: 0.1737 - val_f1_score: 0.1622 - val_precision: 0.1917 - val_recall: 0.1441\n",
      "Epoch 3/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 2.4484 - acc: 0.3146 - f1_score: 0.2399 - precision: 0.5350 - recall: 0.1616 - val_loss: 3.1541 - val_acc: 0.3136 - val_f1_score: 0.3029 - val_precision: 0.4076 - val_recall: 0.2458\n",
      "Epoch 4/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 1.9477 - acc: 0.4447 - f1_score: 0.4004 - precision: 0.7173 - recall: 0.2916 - val_loss: 3.7258 - val_acc: 0.2881 - val_f1_score: 0.2755 - val_precision: 0.3904 - val_recall: 0.2203\n",
      "Epoch 5/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 1.7495 - acc: 0.4885 - f1_score: 0.4710 - precision: 0.7151 - recall: 0.3655 - val_loss: 2.1526 - val_acc: 0.4534 - val_f1_score: 0.4327 - val_precision: 0.6721 - val_recall: 0.3305\n",
      "Epoch 6/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 1.4641 - acc: 0.5723 - f1_score: 0.5548 - precision: 0.7533 - recall: 0.4512 - val_loss: 2.1388 - val_acc: 0.4492 - val_f1_score: 0.3710 - val_precision: 0.5628 - val_recall: 0.2881\n",
      "Epoch 7/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 1.3030 - acc: 0.6203 - f1_score: 0.5986 - precision: 0.7675 - recall: 0.5064 - val_loss: 1.4867 - val_acc: 0.5678 - val_f1_score: 0.5528 - val_precision: 0.6835 - val_recall: 0.4746\n",
      "Epoch 8/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 1.1629 - acc: 0.6552 - f1_score: 0.6409 - precision: 0.7864 - recall: 0.5535 - val_loss: 1.0522 - val_acc: 0.7161 - val_f1_score: 0.6934 - val_precision: 0.8007 - val_recall: 0.6229\n",
      "Epoch 9/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 1.0364 - acc: 0.6919 - f1_score: 0.6863 - precision: 0.8030 - recall: 0.6095 - val_loss: 2.9047 - val_acc: 0.4661 - val_f1_score: 0.4760 - val_precision: 0.5382 - val_recall: 0.4322\n",
      "Epoch 10/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.9328 - acc: 0.7221 - f1_score: 0.7159 - precision: 0.8125 - recall: 0.6510 - val_loss: 1.8163 - val_acc: 0.5975 - val_f1_score: 0.6050 - val_precision: 0.6822 - val_recall: 0.5508\n",
      "Epoch 11/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.8251 - acc: 0.7480 - f1_score: 0.7424 - precision: 0.8291 - recall: 0.6816 - val_loss: 1.3278 - val_acc: 0.6737 - val_f1_score: 0.6857 - val_precision: 0.7412 - val_recall: 0.6441\n",
      "Epoch 12/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.8123 - acc: 0.7532 - f1_score: 0.7480 - precision: 0.8226 - recall: 0.6934 - val_loss: 1.2177 - val_acc: 0.7076 - val_f1_score: 0.7004 - val_precision: 0.7461 - val_recall: 0.6653\n",
      "Epoch 13/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.7779 - acc: 0.7697 - f1_score: 0.7616 - precision: 0.8348 - recall: 0.7075 - val_loss: 1.2787 - val_acc: 0.6314 - val_f1_score: 0.6444 - val_precision: 0.6999 - val_recall: 0.6017\n",
      "Epoch 14/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.7016 - acc: 0.7880 - f1_score: 0.7805 - precision: 0.8486 - recall: 0.7296 - val_loss: 0.8843 - val_acc: 0.7627 - val_f1_score: 0.7670 - val_precision: 0.8222 - val_recall: 0.7246\n",
      "Epoch 15/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.6032 - acc: 0.8187 - f1_score: 0.8161 - precision: 0.8690 - recall: 0.7748 - val_loss: 0.7779 - val_acc: 0.7839 - val_f1_score: 0.7903 - val_precision: 0.8297 - val_recall: 0.7585\n",
      "Epoch 16/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.5890 - acc: 0.8224 - f1_score: 0.8256 - precision: 0.8699 - recall: 0.7904 - val_loss: 1.1359 - val_acc: 0.7415 - val_f1_score: 0.7252 - val_precision: 0.7659 - val_recall: 0.6949\n",
      "Epoch 17/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.5309 - acc: 0.8389 - f1_score: 0.8358 - precision: 0.8774 - recall: 0.8026 - val_loss: 0.8551 - val_acc: 0.7712 - val_f1_score: 0.7747 - val_precision: 0.8156 - val_recall: 0.7415\n",
      "Epoch 18/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.5459 - acc: 0.8394 - f1_score: 0.8323 - precision: 0.8815 - recall: 0.7946 - val_loss: 0.9633 - val_acc: 0.7585 - val_f1_score: 0.7589 - val_precision: 0.7971 - val_recall: 0.7288\n",
      "Epoch 19/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.4379 - acc: 0.8691 - f1_score: 0.8637 - precision: 0.9008 - recall: 0.8347 - val_loss: 1.4472 - val_acc: 0.6864 - val_f1_score: 0.6891 - val_precision: 0.7262 - val_recall: 0.6610\n",
      "Epoch 20/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.4013 - acc: 0.8846 - f1_score: 0.8819 - precision: 0.9123 - recall: 0.8577 - val_loss: 0.9203 - val_acc: 0.7797 - val_f1_score: 0.7860 - val_precision: 0.8136 - val_recall: 0.7627\n",
      "Epoch 21/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.4691 - acc: 0.8573 - f1_score: 0.8544 - precision: 0.8896 - recall: 0.8262 - val_loss: 1.4211 - val_acc: 0.6653 - val_f1_score: 0.6565 - val_precision: 0.6818 - val_recall: 0.6356\n",
      "Epoch 22/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.3082 - acc: 0.9058 - f1_score: 0.9023 - precision: 0.9235 - recall: 0.8846 - val_loss: 1.3909 - val_acc: 0.7246 - val_f1_score: 0.7318 - val_precision: 0.7615 - val_recall: 0.7076\n",
      "Epoch 23/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.3469 - acc: 0.9006 - f1_score: 0.8999 - precision: 0.9197 - recall: 0.8832 - val_loss: 1.4034 - val_acc: 0.7076 - val_f1_score: 0.7133 - val_precision: 0.7421 - val_recall: 0.6907\n",
      "Epoch 24/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.3484 - acc: 0.8997 - f1_score: 0.9016 - precision: 0.9270 - recall: 0.8804 - val_loss: 0.7432 - val_acc: 0.8136 - val_f1_score: 0.8140 - val_precision: 0.8398 - val_recall: 0.7924\n",
      "Epoch 25/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.3300 - acc: 0.9053 - f1_score: 0.9043 - precision: 0.9245 - recall: 0.8874 - val_loss: 0.6347 - val_acc: 0.8263 - val_f1_score: 0.8293 - val_precision: 0.8481 - val_recall: 0.8136\n",
      "Epoch 26/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.2578 - acc: 0.9256 - f1_score: 0.9248 - precision: 0.9412 - recall: 0.9110 - val_loss: 0.8248 - val_acc: 0.8136 - val_f1_score: 0.8243 - val_precision: 0.8414 - val_recall: 0.8093\n",
      "Epoch 27/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.2173 - acc: 0.9350 - f1_score: 0.9367 - precision: 0.9521 - recall: 0.9237 - val_loss: 0.7442 - val_acc: 0.8475 - val_f1_score: 0.8426 - val_precision: 0.8569 - val_recall: 0.8305\n",
      "Epoch 28/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.3089 - acc: 0.9119 - f1_score: 0.9099 - precision: 0.9276 - recall: 0.8950 - val_loss: 1.0800 - val_acc: 0.7754 - val_f1_score: 0.7725 - val_precision: 0.8053 - val_recall: 0.7458\n",
      "Epoch 29/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.2336 - acc: 0.9331 - f1_score: 0.9294 - precision: 0.9437 - recall: 0.9176 - val_loss: 0.9337 - val_acc: 0.8178 - val_f1_score: 0.8206 - val_precision: 0.8384 - val_recall: 0.8051\n",
      "Epoch 30/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.3101 - acc: 0.9195 - f1_score: 0.9174 - precision: 0.9379 - recall: 0.9001 - val_loss: 1.9624 - val_acc: 0.6653 - val_f1_score: 0.6565 - val_precision: 0.6879 - val_recall: 0.6314\n",
      "Epoch 31/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.2233 - acc: 0.9397 - f1_score: 0.9403 - precision: 0.9523 - recall: 0.9303 - val_loss: 0.7628 - val_acc: 0.8347 - val_f1_score: 0.8287 - val_precision: 0.8588 - val_recall: 0.8051\n",
      "Epoch 32/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.2138 - acc: 0.9425 - f1_score: 0.9426 - precision: 0.9543 - recall: 0.9331 - val_loss: 1.0453 - val_acc: 0.7669 - val_f1_score: 0.7739 - val_precision: 0.7988 - val_recall: 0.7542\n",
      "Epoch 33/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1706 - acc: 0.9454 - f1_score: 0.9443 - precision: 0.9554 - recall: 0.9350 - val_loss: 0.6776 - val_acc: 0.8475 - val_f1_score: 0.8506 - val_precision: 0.8545 - val_recall: 0.8475\n",
      "Epoch 34/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1540 - acc: 0.9515 - f1_score: 0.9525 - precision: 0.9608 - recall: 0.9454 - val_loss: 0.9073 - val_acc: 0.7924 - val_f1_score: 0.8017 - val_precision: 0.8178 - val_recall: 0.7881\n",
      "Epoch 35/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1631 - acc: 0.9543 - f1_score: 0.9540 - precision: 0.9604 - recall: 0.9487 - val_loss: 0.9624 - val_acc: 0.8008 - val_f1_score: 0.8124 - val_precision: 0.8305 - val_recall: 0.7966\n",
      "Epoch 36/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1624 - acc: 0.9505 - f1_score: 0.9497 - precision: 0.9603 - recall: 0.9407 - val_loss: 1.0167 - val_acc: 0.8008 - val_f1_score: 0.8205 - val_precision: 0.8485 - val_recall: 0.7966\n",
      "Epoch 37/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1998 - acc: 0.9520 - f1_score: 0.9495 - precision: 0.9598 - recall: 0.9407 - val_loss: 1.1407 - val_acc: 0.7754 - val_f1_score: 0.7818 - val_precision: 0.8091 - val_recall: 0.7627\n",
      "Epoch 38/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.2479 - acc: 0.9411 - f1_score: 0.9389 - precision: 0.9505 - recall: 0.9293 - val_loss: 0.6170 - val_acc: 0.8729 - val_f1_score: 0.8752 - val_precision: 0.8884 - val_recall: 0.8644\n",
      "Epoch 39/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1566 - acc: 0.9548 - f1_score: 0.9553 - precision: 0.9635 - recall: 0.9482 - val_loss: 0.6003 - val_acc: 0.8559 - val_f1_score: 0.8609 - val_precision: 0.8872 - val_recall: 0.8390\n",
      "Epoch 40/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1400 - acc: 0.9590 - f1_score: 0.9582 - precision: 0.9633 - recall: 0.9538 - val_loss: 0.9250 - val_acc: 0.8136 - val_f1_score: 0.8189 - val_precision: 0.8251 - val_recall: 0.8136\n",
      "Epoch 41/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1097 - acc: 0.9642 - f1_score: 0.9653 - precision: 0.9714 - recall: 0.9600 - val_loss: 1.2905 - val_acc: 0.7881 - val_f1_score: 0.7954 - val_precision: 0.8045 - val_recall: 0.7881\n",
      "Epoch 42/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1465 - acc: 0.9590 - f1_score: 0.9577 - precision: 0.9643 - recall: 0.9520 - val_loss: 0.8785 - val_acc: 0.8305 - val_f1_score: 0.8266 - val_precision: 0.8366 - val_recall: 0.8178\n",
      "Epoch 43/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1129 - acc: 0.9647 - f1_score: 0.9636 - precision: 0.9678 - recall: 0.9600 - val_loss: 0.6905 - val_acc: 0.8432 - val_f1_score: 0.8455 - val_precision: 0.8636 - val_recall: 0.8305\n",
      "Epoch 44/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0876 - acc: 0.9717 - f1_score: 0.9720 - precision: 0.9771 - recall: 0.9675 - val_loss: 0.7490 - val_acc: 0.8771 - val_f1_score: 0.8847 - val_precision: 0.8935 - val_recall: 0.8771\n",
      "Epoch 45/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0716 - acc: 0.9802 - f1_score: 0.9810 - precision: 0.9840 - recall: 0.9783 - val_loss: 0.9632 - val_acc: 0.8347 - val_f1_score: 0.8404 - val_precision: 0.8620 - val_recall: 0.8220\n",
      "Epoch 46/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1469 - acc: 0.9595 - f1_score: 0.9596 - precision: 0.9631 - recall: 0.9567 - val_loss: 1.3901 - val_acc: 0.7415 - val_f1_score: 0.7429 - val_precision: 0.7591 - val_recall: 0.7288\n",
      "Epoch 47/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1592 - acc: 0.9595 - f1_score: 0.9596 - precision: 0.9672 - recall: 0.9534 - val_loss: 0.6175 - val_acc: 0.8771 - val_f1_score: 0.8822 - val_precision: 0.8880 - val_recall: 0.8771\n",
      "Epoch 48/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1064 - acc: 0.9713 - f1_score: 0.9707 - precision: 0.9760 - recall: 0.9661 - val_loss: 0.4896 - val_acc: 0.8941 - val_f1_score: 0.8945 - val_precision: 0.9146 - val_recall: 0.8771\n",
      "Epoch 49/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0625 - acc: 0.9793 - f1_score: 0.9807 - precision: 0.9829 - recall: 0.9788 - val_loss: 0.6674 - val_acc: 0.8729 - val_f1_score: 0.8734 - val_precision: 0.8789 - val_recall: 0.8686\n",
      "Epoch 50/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0876 - acc: 0.9755 - f1_score: 0.9755 - precision: 0.9788 - recall: 0.9727 - val_loss: 0.5913 - val_acc: 0.8898 - val_f1_score: 0.8887 - val_precision: 0.8971 - val_recall: 0.8814\n",
      "Epoch 51/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0924 - acc: 0.9760 - f1_score: 0.9759 - precision: 0.9774 - recall: 0.9746 - val_loss: 0.8364 - val_acc: 0.8475 - val_f1_score: 0.8537 - val_precision: 0.8608 - val_recall: 0.8475\n",
      "Epoch 52/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0630 - acc: 0.9849 - f1_score: 0.9835 - precision: 0.9863 - recall: 0.9812 - val_loss: 0.7503 - val_acc: 0.8771 - val_f1_score: 0.8763 - val_precision: 0.8801 - val_recall: 0.8729\n",
      "Epoch 53/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0927 - acc: 0.9769 - f1_score: 0.9771 - precision: 0.9807 - recall: 0.9741 - val_loss: 0.7165 - val_acc: 0.8814 - val_f1_score: 0.8768 - val_precision: 0.8862 - val_recall: 0.8686\n",
      "Epoch 54/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1182 - acc: 0.9755 - f1_score: 0.9746 - precision: 0.9778 - recall: 0.9717 - val_loss: 0.8053 - val_acc: 0.8644 - val_f1_score: 0.8731 - val_precision: 0.8838 - val_recall: 0.8644\n",
      "Epoch 55/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1323 - acc: 0.9637 - f1_score: 0.9639 - precision: 0.9705 - recall: 0.9581 - val_loss: 1.3483 - val_acc: 0.8051 - val_f1_score: 0.8076 - val_precision: 0.8202 - val_recall: 0.7966\n",
      "Epoch 56/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0760 - acc: 0.9774 - f1_score: 0.9768 - precision: 0.9793 - recall: 0.9746 - val_loss: 0.6982 - val_acc: 0.8856 - val_f1_score: 0.8842 - val_precision: 0.8923 - val_recall: 0.8771\n",
      "Epoch 57/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0534 - acc: 0.9882 - f1_score: 0.9878 - precision: 0.9896 - recall: 0.9863 - val_loss: 0.7586 - val_acc: 0.8814 - val_f1_score: 0.8825 - val_precision: 0.8886 - val_recall: 0.8771\n",
      "Epoch 58/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0857 - acc: 0.9774 - f1_score: 0.9776 - precision: 0.9807 - recall: 0.9750 - val_loss: 0.8056 - val_acc: 0.8644 - val_f1_score: 0.8642 - val_precision: 0.8741 - val_recall: 0.8559\n",
      "Epoch 59/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0730 - acc: 0.9779 - f1_score: 0.9772 - precision: 0.9792 - recall: 0.9755 - val_loss: 0.6999 - val_acc: 0.8983 - val_f1_score: 0.8996 - val_precision: 0.9062 - val_recall: 0.8941\n",
      "Epoch 60/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0831 - acc: 0.9826 - f1_score: 0.9824 - precision: 0.9848 - recall: 0.9802 - val_loss: 0.5594 - val_acc: 0.8771 - val_f1_score: 0.8863 - val_precision: 0.8971 - val_recall: 0.8771\n",
      "Epoch 61/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0552 - acc: 0.9830 - f1_score: 0.9833 - precision: 0.9847 - recall: 0.9821 - val_loss: 0.5771 - val_acc: 0.8771 - val_f1_score: 0.8802 - val_precision: 0.8886 - val_recall: 0.8729\n",
      "Epoch 62/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1086 - acc: 0.9746 - f1_score: 0.9733 - precision: 0.9762 - recall: 0.9708 - val_loss: 1.2241 - val_acc: 0.8051 - val_f1_score: 0.8018 - val_precision: 0.8081 - val_recall: 0.7966\n",
      "Epoch 63/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0732 - acc: 0.9830 - f1_score: 0.9825 - precision: 0.9853 - recall: 0.9802 - val_loss: 0.6103 - val_acc: 0.8856 - val_f1_score: 0.8859 - val_precision: 0.8959 - val_recall: 0.8771\n",
      "Epoch 64/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0484 - acc: 0.9849 - f1_score: 0.9850 - precision: 0.9877 - recall: 0.9826 - val_loss: 0.7342 - val_acc: 0.8559 - val_f1_score: 0.8541 - val_precision: 0.8620 - val_recall: 0.8475\n",
      "Epoch 65/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0445 - acc: 0.9868 - f1_score: 0.9867 - precision: 0.9881 - recall: 0.9854 - val_loss: 0.7375 - val_acc: 0.8686 - val_f1_score: 0.8741 - val_precision: 0.8808 - val_recall: 0.8686\n",
      "Epoch 66/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0438 - acc: 0.9854 - f1_score: 0.9848 - precision: 0.9862 - recall: 0.9835 - val_loss: 0.7369 - val_acc: 0.8432 - val_f1_score: 0.8437 - val_precision: 0.8656 - val_recall: 0.8263\n",
      "Epoch 67/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0445 - acc: 0.9849 - f1_score: 0.9842 - precision: 0.9867 - recall: 0.9821 - val_loss: 0.4902 - val_acc: 0.9068 - val_f1_score: 0.9088 - val_precision: 0.9110 - val_recall: 0.9068\n",
      "Epoch 68/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0193 - acc: 0.9934 - f1_score: 0.9937 - precision: 0.9953 - recall: 0.9925 - val_loss: 0.5587 - val_acc: 0.9110 - val_f1_score: 0.9119 - val_precision: 0.9274 - val_recall: 0.8983\n",
      "Epoch 69/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0426 - acc: 0.9887 - f1_score: 0.9889 - precision: 0.9896 - recall: 0.9882 - val_loss: 0.9224 - val_acc: 0.8602 - val_f1_score: 0.8655 - val_precision: 0.8717 - val_recall: 0.8602\n",
      "Epoch 70/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0506 - acc: 0.9859 - f1_score: 0.9869 - precision: 0.9887 - recall: 0.9854 - val_loss: 0.7978 - val_acc: 0.8814 - val_f1_score: 0.8867 - val_precision: 0.8983 - val_recall: 0.8771\n",
      "Epoch 71/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0395 - acc: 0.9892 - f1_score: 0.9886 - precision: 0.9901 - recall: 0.9873 - val_loss: 0.8340 - val_acc: 0.8814 - val_f1_score: 0.8757 - val_precision: 0.8838 - val_recall: 0.8686\n",
      "Epoch 72/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0395 - acc: 0.9892 - f1_score: 0.9898 - precision: 0.9906 - recall: 0.9892 - val_loss: 0.5484 - val_acc: 0.8856 - val_f1_score: 0.8850 - val_precision: 0.8892 - val_recall: 0.8814\n",
      "Epoch 73/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0292 - acc: 0.9939 - f1_score: 0.9936 - precision: 0.9939 - recall: 0.9934 - val_loss: 0.7741 - val_acc: 0.8814 - val_f1_score: 0.8782 - val_precision: 0.8844 - val_recall: 0.8729\n",
      "Epoch 74/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0257 - acc: 0.9920 - f1_score: 0.9921 - precision: 0.9929 - recall: 0.9915 - val_loss: 0.6560 - val_acc: 0.8856 - val_f1_score: 0.8894 - val_precision: 0.9038 - val_recall: 0.8771\n",
      "Epoch 75/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0272 - acc: 0.9943 - f1_score: 0.9943 - precision: 0.9952 - recall: 0.9934 - val_loss: 0.7057 - val_acc: 0.9110 - val_f1_score: 0.9110 - val_precision: 0.9110 - val_recall: 0.9110\n",
      "Epoch 76/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0470 - acc: 0.9901 - f1_score: 0.9893 - precision: 0.9901 - recall: 0.9887 - val_loss: 0.7149 - val_acc: 0.9068 - val_f1_score: 0.9085 - val_precision: 0.9104 - val_recall: 0.9068\n",
      "Epoch 77/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1083 - acc: 0.9708 - f1_score: 0.9719 - precision: 0.9749 - recall: 0.9694 - val_loss: 1.5824 - val_acc: 0.7754 - val_f1_score: 0.7777 - val_precision: 0.7900 - val_recall: 0.7669\n",
      "Epoch 78/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.1029 - acc: 0.9708 - f1_score: 0.9716 - precision: 0.9758 - recall: 0.9680 - val_loss: 0.8650 - val_acc: 0.8220 - val_f1_score: 0.8213 - val_precision: 0.8305 - val_recall: 0.8136\n",
      "Epoch 79/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0474 - acc: 0.9863 - f1_score: 0.9859 - precision: 0.9887 - recall: 0.9835 - val_loss: 0.9659 - val_acc: 0.8686 - val_f1_score: 0.8681 - val_precision: 0.8723 - val_recall: 0.8644\n",
      "Epoch 80/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0456 - acc: 0.9868 - f1_score: 0.9869 - precision: 0.9886 - recall: 0.9854 - val_loss: 0.8942 - val_acc: 0.8559 - val_f1_score: 0.8580 - val_precision: 0.8656 - val_recall: 0.8517\n",
      "Epoch 81/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0471 - acc: 0.9887 - f1_score: 0.9874 - precision: 0.9887 - recall: 0.9863 - val_loss: 0.8812 - val_acc: 0.8771 - val_f1_score: 0.8763 - val_precision: 0.8801 - val_recall: 0.8729\n",
      "Epoch 82/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0430 - acc: 0.9882 - f1_score: 0.9887 - precision: 0.9892 - recall: 0.9882 - val_loss: 0.8542 - val_acc: 0.8686 - val_f1_score: 0.8675 - val_precision: 0.8711 - val_recall: 0.8644\n",
      "Epoch 83/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0640 - acc: 0.9830 - f1_score: 0.9836 - precision: 0.9859 - recall: 0.9816 - val_loss: 0.7351 - val_acc: 0.8602 - val_f1_score: 0.8590 - val_precision: 0.8682 - val_recall: 0.8517\n",
      "Epoch 84/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0291 - acc: 0.9934 - f1_score: 0.9927 - precision: 0.9934 - recall: 0.9920 - val_loss: 1.0206 - val_acc: 0.8432 - val_f1_score: 0.8454 - val_precision: 0.8533 - val_recall: 0.8390\n",
      "Epoch 85/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0239 - acc: 0.9953 - f1_score: 0.9948 - precision: 0.9953 - recall: 0.9943 - val_loss: 0.5764 - val_acc: 0.8983 - val_f1_score: 0.8949 - val_precision: 0.9007 - val_recall: 0.8898\n",
      "Epoch 86/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0078 - acc: 0.9981 - f1_score: 0.9979 - precision: 0.9981 - recall: 0.9976 - val_loss: 0.9781 - val_acc: 0.8475 - val_f1_score: 0.8449 - val_precision: 0.8469 - val_recall: 0.8432\n",
      "Epoch 87/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0129 - acc: 0.9958 - f1_score: 0.9960 - precision: 0.9962 - recall: 0.9958 - val_loss: 0.8122 - val_acc: 0.8686 - val_f1_score: 0.8720 - val_precision: 0.8759 - val_recall: 0.8686\n",
      "Epoch 88/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0191 - acc: 0.9967 - f1_score: 0.9959 - precision: 0.9967 - recall: 0.9953 - val_loss: 0.6732 - val_acc: 0.9068 - val_f1_score: 0.9085 - val_precision: 0.9104 - val_recall: 0.9068\n",
      "Epoch 89/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0492 - acc: 0.9859 - f1_score: 0.9857 - precision: 0.9872 - recall: 0.9845 - val_loss: 0.7648 - val_acc: 0.8856 - val_f1_score: 0.8873 - val_precision: 0.8892 - val_recall: 0.8856\n",
      "Epoch 90/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0408 - acc: 0.9882 - f1_score: 0.9879 - precision: 0.9887 - recall: 0.9873 - val_loss: 0.6174 - val_acc: 0.8983 - val_f1_score: 0.8983 - val_precision: 0.9080 - val_recall: 0.8898\n",
      "Epoch 91/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0418 - acc: 0.9887 - f1_score: 0.9883 - precision: 0.9900 - recall: 0.9868 - val_loss: 1.1325 - val_acc: 0.8432 - val_f1_score: 0.8446 - val_precision: 0.8511 - val_recall: 0.8390\n",
      "Epoch 92/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0235 - acc: 0.9925 - f1_score: 0.9924 - precision: 0.9929 - recall: 0.9920 - val_loss: 0.7696 - val_acc: 0.8856 - val_f1_score: 0.8887 - val_precision: 0.8923 - val_recall: 0.8856\n",
      "Epoch 93/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0376 - acc: 0.9915 - f1_score: 0.9917 - precision: 0.9919 - recall: 0.9915 - val_loss: 0.9793 - val_acc: 0.8475 - val_f1_score: 0.8475 - val_precision: 0.8571 - val_recall: 0.8390\n",
      "Epoch 94/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0647 - acc: 0.9802 - f1_score: 0.9798 - precision: 0.9810 - recall: 0.9788 - val_loss: 0.6482 - val_acc: 0.9068 - val_f1_score: 0.9100 - val_precision: 0.9237 - val_recall: 0.8983\n",
      "Epoch 95/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0172 - acc: 0.9939 - f1_score: 0.9938 - precision: 0.9948 - recall: 0.9929 - val_loss: 0.6330 - val_acc: 0.8983 - val_f1_score: 0.9020 - val_precision: 0.9062 - val_recall: 0.8983\n",
      "Epoch 96/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0462 - acc: 0.9873 - f1_score: 0.9878 - precision: 0.9896 - recall: 0.9863 - val_loss: 0.9409 - val_acc: 0.8729 - val_f1_score: 0.8766 - val_precision: 0.8808 - val_recall: 0.8729\n",
      "Epoch 97/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0424 - acc: 0.9878 - f1_score: 0.9870 - precision: 0.9878 - recall: 0.9863 - val_loss: 0.8852 - val_acc: 0.8644 - val_f1_score: 0.8602 - val_precision: 0.8699 - val_recall: 0.8517\n",
      "Epoch 98/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0230 - acc: 0.9925 - f1_score: 0.9922 - precision: 0.9929 - recall: 0.9915 - val_loss: 0.6499 - val_acc: 0.8771 - val_f1_score: 0.8782 - val_precision: 0.8844 - val_recall: 0.8729\n",
      "Epoch 99/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0344 - acc: 0.9887 - f1_score: 0.9890 - precision: 0.9900 - recall: 0.9882 - val_loss: 0.8623 - val_acc: 0.8898 - val_f1_score: 0.8847 - val_precision: 0.8886 - val_recall: 0.8814\n",
      "Epoch 100/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0243 - acc: 0.9920 - f1_score: 0.9924 - precision: 0.9934 - recall: 0.9915 - val_loss: 0.6326 - val_acc: 0.9110 - val_f1_score: 0.9130 - val_precision: 0.9153 - val_recall: 0.9110\n",
      "Epoch 101/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0204 - acc: 0.9958 - f1_score: 0.9959 - precision: 0.9972 - recall: 0.9948 - val_loss: 0.6548 - val_acc: 0.8983 - val_f1_score: 0.8956 - val_precision: 0.9025 - val_recall: 0.8898\n",
      "Epoch 102/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0365 - acc: 0.9882 - f1_score: 0.9883 - precision: 0.9896 - recall: 0.9873 - val_loss: 0.6514 - val_acc: 0.8856 - val_f1_score: 0.8876 - val_precision: 0.8898 - val_recall: 0.8856\n",
      "Epoch 103/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0779 - acc: 0.9812 - f1_score: 0.9809 - precision: 0.9839 - recall: 0.9783 - val_loss: 0.6824 - val_acc: 0.8983 - val_f1_score: 0.8953 - val_precision: 0.9019 - val_recall: 0.8898\n",
      "Epoch 104/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0176 - acc: 0.9934 - f1_score: 0.9936 - precision: 0.9943 - recall: 0.9929 - val_loss: 0.6983 - val_acc: 0.9068 - val_f1_score: 0.9065 - val_precision: 0.9110 - val_recall: 0.9025\n",
      "Epoch 105/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0214 - acc: 0.9925 - f1_score: 0.9927 - precision: 0.9929 - recall: 0.9925 - val_loss: 0.6596 - val_acc: 0.8941 - val_f1_score: 0.8949 - val_precision: 0.9007 - val_recall: 0.8898\n",
      "Epoch 106/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0392 - acc: 0.9887 - f1_score: 0.9899 - precision: 0.9920 - recall: 0.9882 - val_loss: 0.6387 - val_acc: 0.8941 - val_f1_score: 0.8994 - val_precision: 0.9056 - val_recall: 0.8941\n",
      "Epoch 107/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0195 - acc: 0.9948 - f1_score: 0.9948 - precision: 0.9953 - recall: 0.9943 - val_loss: 0.7542 - val_acc: 0.8856 - val_f1_score: 0.8822 - val_precision: 0.8880 - val_recall: 0.8771\n",
      "Epoch 108/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0434 - acc: 0.9915 - f1_score: 0.9922 - precision: 0.9929 - recall: 0.9915 - val_loss: 0.8982 - val_acc: 0.8644 - val_f1_score: 0.8684 - val_precision: 0.8729 - val_recall: 0.8644\n",
      "Epoch 109/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0412 - acc: 0.9896 - f1_score: 0.9891 - precision: 0.9896 - recall: 0.9887 - val_loss: 0.9369 - val_acc: 0.8644 - val_f1_score: 0.8644 - val_precision: 0.8644 - val_recall: 0.8644\n",
      "Epoch 110/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0369 - acc: 0.9901 - f1_score: 0.9893 - precision: 0.9900 - recall: 0.9887 - val_loss: 0.9420 - val_acc: 0.8475 - val_f1_score: 0.8528 - val_precision: 0.8590 - val_recall: 0.8475\n",
      "Epoch 111/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0257 - acc: 0.9920 - f1_score: 0.9927 - precision: 0.9934 - recall: 0.9920 - val_loss: 0.9134 - val_acc: 0.9025 - val_f1_score: 0.9017 - val_precision: 0.9056 - val_recall: 0.8983\n",
      "Epoch 112/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0251 - acc: 0.9943 - f1_score: 0.9943 - precision: 0.9948 - recall: 0.9939 - val_loss: 0.7587 - val_acc: 0.8729 - val_f1_score: 0.8743 - val_precision: 0.8759 - val_recall: 0.8729\n",
      "Epoch 113/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0072 - acc: 0.9976 - f1_score: 0.9974 - precision: 0.9976 - recall: 0.9972 - val_loss: 0.6186 - val_acc: 0.9025 - val_f1_score: 0.9023 - val_precision: 0.9068 - val_recall: 0.8983\n",
      "Epoch 114/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0192 - acc: 0.9943 - f1_score: 0.9940 - precision: 0.9948 - recall: 0.9934 - val_loss: 0.5314 - val_acc: 0.9110 - val_f1_score: 0.9110 - val_precision: 0.9110 - val_recall: 0.9110\n",
      "Epoch 115/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0168 - acc: 0.9948 - f1_score: 0.9948 - precision: 0.9953 - recall: 0.9943 - val_loss: 0.7469 - val_acc: 0.8856 - val_f1_score: 0.8893 - val_precision: 0.8935 - val_recall: 0.8856\n",
      "Epoch 116/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0552 - acc: 0.9896 - f1_score: 0.9893 - precision: 0.9906 - recall: 0.9882 - val_loss: 0.7126 - val_acc: 0.8856 - val_f1_score: 0.8856 - val_precision: 0.8856 - val_recall: 0.8856\n",
      "Epoch 117/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0395 - acc: 0.9887 - f1_score: 0.9879 - precision: 0.9886 - recall: 0.9873 - val_loss: 0.7770 - val_acc: 0.8856 - val_f1_score: 0.8895 - val_precision: 0.9044 - val_recall: 0.8771\n",
      "Epoch 118/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0229 - acc: 0.9943 - f1_score: 0.9945 - precision: 0.9953 - recall: 0.9939 - val_loss: 0.7264 - val_acc: 0.8941 - val_f1_score: 0.8977 - val_precision: 0.9019 - val_recall: 0.8941\n",
      "Epoch 119/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0070 - acc: 0.9976 - f1_score: 0.9974 - precision: 0.9976 - recall: 0.9972 - val_loss: 0.6085 - val_acc: 0.8983 - val_f1_score: 0.8983 - val_precision: 0.8983 - val_recall: 0.8983\n",
      "Epoch 120/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0118 - acc: 0.9972 - f1_score: 0.9969 - precision: 0.9972 - recall: 0.9967 - val_loss: 0.7094 - val_acc: 0.8814 - val_f1_score: 0.8828 - val_precision: 0.8844 - val_recall: 0.8814\n",
      "Epoch 121/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0328 - acc: 0.9920 - f1_score: 0.9920 - precision: 0.9925 - recall: 0.9915 - val_loss: 0.6054 - val_acc: 0.8856 - val_f1_score: 0.8845 - val_precision: 0.8929 - val_recall: 0.8771\n",
      "Epoch 122/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0063 - acc: 0.9981 - f1_score: 0.9981 - precision: 0.9981 - recall: 0.9981 - val_loss: 0.6663 - val_acc: 0.8686 - val_f1_score: 0.8723 - val_precision: 0.8765 - val_recall: 0.8686\n",
      "Epoch 123/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0048 - acc: 0.9986 - f1_score: 0.9986 - precision: 0.9991 - recall: 0.9981 - val_loss: 0.6928 - val_acc: 0.8898 - val_f1_score: 0.8918 - val_precision: 0.8941 - val_recall: 0.8898\n",
      "Epoch 124/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0163 - acc: 0.9953 - f1_score: 0.9952 - precision: 0.9956 - recall: 0.9948 - val_loss: 0.6132 - val_acc: 0.9110 - val_f1_score: 0.9085 - val_precision: 0.9104 - val_recall: 0.9068\n",
      "Epoch 125/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0265 - acc: 0.9943 - f1_score: 0.9945 - precision: 0.9953 - recall: 0.9939 - val_loss: 0.4876 - val_acc: 0.9110 - val_f1_score: 0.9198 - val_precision: 0.9298 - val_recall: 0.9110\n",
      "Epoch 126/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0304 - acc: 0.9939 - f1_score: 0.9940 - precision: 0.9948 - recall: 0.9934 - val_loss: 0.6441 - val_acc: 0.8814 - val_f1_score: 0.8813 - val_precision: 0.8914 - val_recall: 0.8729\n",
      "Epoch 127/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0435 - acc: 0.9925 - f1_score: 0.9927 - precision: 0.9929 - recall: 0.9925 - val_loss: 0.8283 - val_acc: 0.8771 - val_f1_score: 0.8720 - val_precision: 0.8862 - val_recall: 0.8602\n",
      "Epoch 128/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0178 - acc: 0.9925 - f1_score: 0.9927 - precision: 0.9934 - recall: 0.9920 - val_loss: 0.7851 - val_acc: 0.8898 - val_f1_score: 0.8887 - val_precision: 0.8977 - val_recall: 0.8814\n",
      "Epoch 129/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0410 - acc: 0.9925 - f1_score: 0.9922 - precision: 0.9925 - recall: 0.9920 - val_loss: 0.8037 - val_acc: 0.8686 - val_f1_score: 0.8726 - val_precision: 0.8874 - val_recall: 0.8602\n",
      "Epoch 130/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0128 - acc: 0.9943 - f1_score: 0.9945 - precision: 0.9958 - recall: 0.9934 - val_loss: 0.6401 - val_acc: 0.9025 - val_f1_score: 0.9000 - val_precision: 0.9019 - val_recall: 0.8983\n",
      "Epoch 131/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0439 - acc: 0.9911 - f1_score: 0.9905 - precision: 0.9915 - recall: 0.9896 - val_loss: 0.5194 - val_acc: 0.9110 - val_f1_score: 0.9167 - val_precision: 0.9231 - val_recall: 0.9110\n",
      "Epoch 132/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0058 - acc: 0.9976 - f1_score: 0.9979 - precision: 0.9981 - recall: 0.9976 - val_loss: 0.5889 - val_acc: 0.8983 - val_f1_score: 0.9000 - val_precision: 0.9019 - val_recall: 0.8983\n",
      "Epoch 133/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0029 - acc: 0.9991 - f1_score: 0.9988 - precision: 0.9991 - recall: 0.9986 - val_loss: 0.5092 - val_acc: 0.9110 - val_f1_score: 0.9127 - val_precision: 0.9146 - val_recall: 0.9110\n",
      "Epoch 134/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0165 - acc: 0.9953 - f1_score: 0.9957 - precision: 0.9967 - recall: 0.9953 - val_loss: 0.7169 - val_acc: 0.8898 - val_f1_score: 0.8966 - val_precision: 0.9044 - val_recall: 0.8898\n",
      "Epoch 135/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0032 - acc: 0.9991 - f1_score: 0.9984 - precision: 0.9991 - recall: 0.9981 - val_loss: 0.5754 - val_acc: 0.9110 - val_f1_score: 0.9105 - val_precision: 0.9146 - val_recall: 0.9068\n",
      "Epoch 136/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0252 - acc: 0.9939 - f1_score: 0.9943 - precision: 0.9948 - recall: 0.9939 - val_loss: 0.5750 - val_acc: 0.8941 - val_f1_score: 0.9031 - val_precision: 0.9134 - val_recall: 0.8941\n",
      "Epoch 137/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0079 - acc: 0.9962 - f1_score: 0.9965 - precision: 0.9967 - recall: 0.9962 - val_loss: 0.5854 - val_acc: 0.9025 - val_f1_score: 0.9040 - val_precision: 0.9104 - val_recall: 0.8983\n",
      "Epoch 138/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0068 - acc: 0.9962 - f1_score: 0.9962 - precision: 0.9962 - recall: 0.9962 - val_loss: 0.6852 - val_acc: 0.8898 - val_f1_score: 0.8904 - val_precision: 0.8959 - val_recall: 0.8856\n",
      "Epoch 139/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0024 - acc: 0.9991 - f1_score: 0.9993 - precision: 0.9995 - recall: 0.9991 - val_loss: 0.5856 - val_acc: 0.9068 - val_f1_score: 0.9073 - val_precision: 0.9177 - val_recall: 0.8983\n",
      "Epoch 140/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0141 - acc: 0.9967 - f1_score: 0.9967 - precision: 0.9967 - recall: 0.9967 - val_loss: 0.5490 - val_acc: 0.9195 - val_f1_score: 0.9164 - val_precision: 0.9225 - val_recall: 0.9110\n",
      "Epoch 141/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0031 - acc: 0.9981 - f1_score: 0.9983 - precision: 0.9986 - recall: 0.9981 - val_loss: 0.5026 - val_acc: 0.9025 - val_f1_score: 0.9039 - val_precision: 0.9110 - val_recall: 0.8983\n",
      "Epoch 142/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0022 - acc: 0.9991 - f1_score: 0.9991 - precision: 0.9991 - recall: 0.9991 - val_loss: 0.4566 - val_acc: 0.9237 - val_f1_score: 0.9220 - val_precision: 0.9298 - val_recall: 0.9153\n",
      "Epoch 143/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0018 - acc: 0.9991 - f1_score: 0.9988 - precision: 0.9991 - recall: 0.9986 - val_loss: 0.4516 - val_acc: 0.9237 - val_f1_score: 0.9237 - val_precision: 0.9237 - val_recall: 0.9237\n",
      "Epoch 144/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0041 - acc: 0.9991 - f1_score: 0.9990 - precision: 0.9995 - recall: 0.9986 - val_loss: 0.5462 - val_acc: 0.9025 - val_f1_score: 0.9025 - val_precision: 0.9025 - val_recall: 0.9025\n",
      "Epoch 145/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0282 - acc: 0.9929 - f1_score: 0.9932 - precision: 0.9934 - recall: 0.9929 - val_loss: 0.5955 - val_acc: 0.8856 - val_f1_score: 0.8907 - val_precision: 0.8965 - val_recall: 0.8856\n",
      "Epoch 146/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0086 - acc: 0.9976 - f1_score: 0.9976 - precision: 0.9976 - recall: 0.9976 - val_loss: 0.6371 - val_acc: 0.8771 - val_f1_score: 0.8853 - val_precision: 0.8947 - val_recall: 0.8771\n",
      "Epoch 147/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0048 - acc: 0.9981 - f1_score: 0.9983 - precision: 0.9986 - recall: 0.9981 - val_loss: 0.6798 - val_acc: 0.9025 - val_f1_score: 0.9014 - val_precision: 0.9050 - val_recall: 0.8983\n",
      "Epoch 148/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0168 - acc: 0.9958 - f1_score: 0.9962 - precision: 0.9967 - recall: 0.9958 - val_loss: 0.8013 - val_acc: 0.8729 - val_f1_score: 0.8766 - val_precision: 0.8808 - val_recall: 0.8729\n",
      "Epoch 149/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0188 - acc: 0.9943 - f1_score: 0.9950 - precision: 0.9958 - recall: 0.9943 - val_loss: 0.6859 - val_acc: 0.9068 - val_f1_score: 0.9105 - val_precision: 0.9146 - val_recall: 0.9068\n",
      "Epoch 150/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0299 - acc: 0.9920 - f1_score: 0.9920 - precision: 0.9920 - recall: 0.9920 - val_loss: 0.6745 - val_acc: 0.8814 - val_f1_score: 0.8839 - val_precision: 0.8916 - val_recall: 0.8771\n",
      "Epoch 151/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0240 - acc: 0.9915 - f1_score: 0.9917 - precision: 0.9929 - recall: 0.9906 - val_loss: 0.8000 - val_acc: 0.8814 - val_f1_score: 0.8788 - val_precision: 0.8808 - val_recall: 0.8771\n",
      "Epoch 152/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0187 - acc: 0.9934 - f1_score: 0.9929 - precision: 0.9934 - recall: 0.9925 - val_loss: 0.7257 - val_acc: 0.8814 - val_f1_score: 0.8799 - val_precision: 0.8880 - val_recall: 0.8729\n",
      "Epoch 153/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0189 - acc: 0.9939 - f1_score: 0.9941 - precision: 0.9943 - recall: 0.9939 - val_loss: 0.5684 - val_acc: 0.8983 - val_f1_score: 0.9037 - val_precision: 0.9098 - val_recall: 0.8983\n",
      "Epoch 154/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0642 - acc: 0.9863 - f1_score: 0.9860 - precision: 0.9872 - recall: 0.9849 - val_loss: 0.8590 - val_acc: 0.8644 - val_f1_score: 0.8635 - val_precision: 0.8826 - val_recall: 0.8475\n",
      "Epoch 155/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0118 - acc: 0.9948 - f1_score: 0.9945 - precision: 0.9957 - recall: 0.9934 - val_loss: 0.6048 - val_acc: 0.9153 - val_f1_score: 0.9141 - val_precision: 0.9225 - val_recall: 0.9068\n",
      "Epoch 156/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0182 - acc: 0.9962 - f1_score: 0.9957 - precision: 0.9962 - recall: 0.9953 - val_loss: 0.6106 - val_acc: 0.9110 - val_f1_score: 0.9099 - val_precision: 0.9183 - val_recall: 0.9025\n",
      "Epoch 157/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0105 - acc: 0.9967 - f1_score: 0.9969 - precision: 0.9976 - recall: 0.9962 - val_loss: 0.4943 - val_acc: 0.9110 - val_f1_score: 0.9137 - val_precision: 0.9219 - val_recall: 0.9068\n",
      "Epoch 158/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0093 - acc: 0.9972 - f1_score: 0.9971 - precision: 0.9976 - recall: 0.9967 - val_loss: 0.5064 - val_acc: 0.8941 - val_f1_score: 0.8992 - val_precision: 0.9050 - val_recall: 0.8941\n",
      "Epoch 159/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0046 - acc: 0.9981 - f1_score: 0.9976 - precision: 0.9981 - recall: 0.9972 - val_loss: 0.5449 - val_acc: 0.9025 - val_f1_score: 0.9008 - val_precision: 0.9086 - val_recall: 0.8941\n",
      "Epoch 160/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0019 - acc: 0.9986 - f1_score: 0.9988 - precision: 0.9991 - recall: 0.9986 - val_loss: 0.5451 - val_acc: 0.9068 - val_f1_score: 0.8989 - val_precision: 0.9092 - val_recall: 0.8898\n",
      "Epoch 161/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0038 - acc: 0.9986 - f1_score: 0.9983 - precision: 0.9986 - recall: 0.9981 - val_loss: 0.4799 - val_acc: 0.9153 - val_f1_score: 0.9107 - val_precision: 0.9153 - val_recall: 0.9068\n",
      "Epoch 162/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0040 - acc: 0.9991 - f1_score: 0.9991 - precision: 0.9991 - recall: 0.9991 - val_loss: 0.4819 - val_acc: 0.8983 - val_f1_score: 0.8955 - val_precision: 0.9019 - val_recall: 0.8898\n",
      "Epoch 163/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0179 - acc: 0.9958 - f1_score: 0.9964 - precision: 0.9972 - recall: 0.9958 - val_loss: 0.5690 - val_acc: 0.8856 - val_f1_score: 0.8921 - val_precision: 0.9050 - val_recall: 0.8814\n",
      "Epoch 164/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0096 - acc: 0.9962 - f1_score: 0.9963 - precision: 0.9965 - recall: 0.9962 - val_loss: 0.5527 - val_acc: 0.9025 - val_f1_score: 0.8980 - val_precision: 0.9025 - val_recall: 0.8941\n",
      "Epoch 165/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0173 - acc: 0.9953 - f1_score: 0.9955 - precision: 0.9958 - recall: 0.9953 - val_loss: 0.6688 - val_acc: 0.9153 - val_f1_score: 0.9168 - val_precision: 0.9237 - val_recall: 0.9110\n",
      "Epoch 166/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0272 - acc: 0.9943 - f1_score: 0.9938 - precision: 0.9943 - recall: 0.9934 - val_loss: 0.7059 - val_acc: 0.8814 - val_f1_score: 0.8797 - val_precision: 0.8874 - val_recall: 0.8729\n",
      "Epoch 167/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0116 - acc: 0.9958 - f1_score: 0.9958 - precision: 0.9958 - recall: 0.9958 - val_loss: 0.9451 - val_acc: 0.8602 - val_f1_score: 0.8678 - val_precision: 0.8765 - val_recall: 0.8602\n",
      "Epoch 168/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0155 - acc: 0.9948 - f1_score: 0.9948 - precision: 0.9948 - recall: 0.9948 - val_loss: 0.7988 - val_acc: 0.8729 - val_f1_score: 0.8803 - val_precision: 0.8892 - val_recall: 0.8729\n",
      "Epoch 169/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0383 - acc: 0.9929 - f1_score: 0.9929 - precision: 0.9934 - recall: 0.9925 - val_loss: 0.7048 - val_acc: 0.8729 - val_f1_score: 0.8763 - val_precision: 0.8801 - val_recall: 0.8729\n",
      "Epoch 170/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0645 - acc: 0.9845 - f1_score: 0.9848 - precision: 0.9863 - recall: 0.9835 - val_loss: 0.8758 - val_acc: 0.8517 - val_f1_score: 0.8511 - val_precision: 0.8553 - val_recall: 0.8475\n",
      "Epoch 171/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0285 - acc: 0.9920 - f1_score: 0.9920 - precision: 0.9925 - recall: 0.9915 - val_loss: 0.8772 - val_acc: 0.8347 - val_f1_score: 0.8415 - val_precision: 0.8493 - val_recall: 0.8347\n",
      "Epoch 172/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0348 - acc: 0.9920 - f1_score: 0.9924 - precision: 0.9929 - recall: 0.9920 - val_loss: 0.8004 - val_acc: 0.8729 - val_f1_score: 0.8757 - val_precision: 0.8838 - val_recall: 0.8686\n",
      "Epoch 173/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0239 - acc: 0.9934 - f1_score: 0.9936 - precision: 0.9939 - recall: 0.9934 - val_loss: 0.6219 - val_acc: 0.8898 - val_f1_score: 0.8956 - val_precision: 0.9025 - val_recall: 0.8898\n",
      "Epoch 174/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0195 - acc: 0.9943 - f1_score: 0.9945 - precision: 0.9953 - recall: 0.9939 - val_loss: 0.6641 - val_acc: 0.8983 - val_f1_score: 0.9011 - val_precision: 0.9092 - val_recall: 0.8941\n",
      "Epoch 175/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0025 - acc: 0.9991 - f1_score: 0.9991 - precision: 0.9991 - recall: 0.9991 - val_loss: 0.6629 - val_acc: 0.8983 - val_f1_score: 0.9015 - val_precision: 0.9104 - val_recall: 0.8941\n",
      "Epoch 176/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0040 - acc: 0.9981 - f1_score: 0.9981 - precision: 0.9981 - recall: 0.9981 - val_loss: 0.7061 - val_acc: 0.9025 - val_f1_score: 0.9063 - val_precision: 0.9110 - val_recall: 0.9025\n",
      "Epoch 177/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0071 - acc: 0.9976 - f1_score: 0.9979 - precision: 0.9981 - recall: 0.9976 - val_loss: 0.6817 - val_acc: 0.9110 - val_f1_score: 0.9082 - val_precision: 0.9146 - val_recall: 0.9025\n",
      "Epoch 178/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0017 - acc: 0.9986 - f1_score: 0.9988 - precision: 0.9991 - recall: 0.9986 - val_loss: 0.7903 - val_acc: 0.8941 - val_f1_score: 0.8996 - val_precision: 0.9062 - val_recall: 0.8941\n",
      "Epoch 179/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0014 - acc: 1.0000 - f1_score: 0.9997 - precision: 1.0000 - recall: 0.9995 - val_loss: 0.8062 - val_acc: 0.8983 - val_f1_score: 0.8977 - val_precision: 0.9019 - val_recall: 0.8941\n",
      "Epoch 180/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0137 - acc: 0.9976 - f1_score: 0.9979 - precision: 0.9981 - recall: 0.9976 - val_loss: 0.6933 - val_acc: 0.9110 - val_f1_score: 0.9110 - val_precision: 0.9110 - val_recall: 0.9110\n",
      "Epoch 181/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0047 - acc: 0.9986 - f1_score: 0.9986 - precision: 0.9986 - recall: 0.9986 - val_loss: 0.7794 - val_acc: 0.9025 - val_f1_score: 0.9045 - val_precision: 0.9068 - val_recall: 0.9025\n",
      "Epoch 182/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0025 - acc: 0.9991 - f1_score: 0.9988 - precision: 0.9991 - recall: 0.9986 - val_loss: 0.8008 - val_acc: 0.8941 - val_f1_score: 0.8915 - val_precision: 0.8935 - val_recall: 0.8898\n",
      "Epoch 183/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0011 - acc: 0.9995 - f1_score: 0.9995 - precision: 0.9995 - recall: 0.9995 - val_loss: 0.7660 - val_acc: 0.8941 - val_f1_score: 0.8941 - val_precision: 0.8941 - val_recall: 0.8941\n",
      "Epoch 184/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0017 - acc: 0.9995 - f1_score: 0.9995 - precision: 0.9995 - recall: 0.9995 - val_loss: 0.6954 - val_acc: 0.9025 - val_f1_score: 0.9025 - val_precision: 0.9025 - val_recall: 0.9025\n",
      "Epoch 185/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0011 - acc: 0.9995 - f1_score: 0.9995 - precision: 0.9995 - recall: 0.9995 - val_loss: 0.7028 - val_acc: 0.9068 - val_f1_score: 0.9042 - val_precision: 0.9062 - val_recall: 0.9025\n",
      "Epoch 186/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0039 - acc: 0.9986 - f1_score: 0.9986 - precision: 0.9986 - recall: 0.9986 - val_loss: 0.6097 - val_acc: 0.9068 - val_f1_score: 0.9105 - val_precision: 0.9146 - val_recall: 0.9068\n",
      "Epoch 187/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0023 - acc: 0.9991 - f1_score: 0.9991 - precision: 0.9991 - recall: 0.9991 - val_loss: 0.6516 - val_acc: 0.9068 - val_f1_score: 0.9045 - val_precision: 0.9068 - val_recall: 0.9025\n",
      "Epoch 188/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0023 - acc: 0.9991 - f1_score: 0.9991 - precision: 0.9991 - recall: 0.9991 - val_loss: 0.7313 - val_acc: 0.8983 - val_f1_score: 0.9035 - val_precision: 0.9098 - val_recall: 0.8983\n",
      "Epoch 189/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0154 - acc: 0.9967 - f1_score: 0.9967 - precision: 0.9967 - recall: 0.9967 - val_loss: 0.6185 - val_acc: 0.9110 - val_f1_score: 0.9082 - val_precision: 0.9146 - val_recall: 0.9025\n",
      "Epoch 190/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0082 - acc: 0.9972 - f1_score: 0.9972 - precision: 0.9972 - recall: 0.9972 - val_loss: 0.7338 - val_acc: 0.9153 - val_f1_score: 0.9107 - val_precision: 0.9153 - val_recall: 0.9068\n",
      "Epoch 191/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0084 - acc: 0.9972 - f1_score: 0.9974 - precision: 0.9976 - recall: 0.9972 - val_loss: 0.6826 - val_acc: 0.9195 - val_f1_score: 0.9215 - val_precision: 0.9237 - val_recall: 0.9195\n",
      "Epoch 192/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0021 - acc: 0.9991 - f1_score: 0.9991 - precision: 0.9991 - recall: 0.9991 - val_loss: 0.6344 - val_acc: 0.9153 - val_f1_score: 0.9188 - val_precision: 0.9231 - val_recall: 0.9153\n",
      "Epoch 193/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0025 - acc: 0.9995 - f1_score: 0.9993 - precision: 0.9995 - recall: 0.9991 - val_loss: 0.6389 - val_acc: 0.9153 - val_f1_score: 0.9171 - val_precision: 0.9195 - val_recall: 0.9153\n",
      "Epoch 194/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0019 - acc: 0.9991 - f1_score: 0.9991 - precision: 0.9991 - recall: 0.9991 - val_loss: 0.6645 - val_acc: 0.9153 - val_f1_score: 0.9169 - val_precision: 0.9189 - val_recall: 0.9153\n",
      "Epoch 195/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0022 - acc: 0.9995 - f1_score: 0.9997 - precision: 1.0000 - recall: 0.9995 - val_loss: 0.6683 - val_acc: 0.9195 - val_f1_score: 0.9195 - val_precision: 0.9195 - val_recall: 0.9195\n",
      "Epoch 196/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0022 - acc: 0.9991 - f1_score: 0.9991 - precision: 0.9991 - recall: 0.9991 - val_loss: 0.6643 - val_acc: 0.9195 - val_f1_score: 0.9195 - val_precision: 0.9195 - val_recall: 0.9195\n",
      "Epoch 197/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0010 - acc: 0.9995 - f1_score: 0.9995 - precision: 0.9995 - recall: 0.9995 - val_loss: 0.6637 - val_acc: 0.9195 - val_f1_score: 0.9213 - val_precision: 0.9237 - val_recall: 0.9195\n",
      "Epoch 198/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0015 - acc: 0.9995 - f1_score: 0.9995 - precision: 0.9995 - recall: 0.9995 - val_loss: 0.6193 - val_acc: 0.9195 - val_f1_score: 0.9195 - val_precision: 0.9195 - val_recall: 0.9195\n",
      "Epoch 199/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0021 - acc: 0.9986 - f1_score: 0.9986 - precision: 0.9986 - recall: 0.9986 - val_loss: 0.7465 - val_acc: 0.8941 - val_f1_score: 0.8955 - val_precision: 0.9019 - val_recall: 0.8898\n",
      "Epoch 200/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0055 - acc: 0.9986 - f1_score: 0.9986 - precision: 0.9986 - recall: 0.9986 - val_loss: 0.6388 - val_acc: 0.9195 - val_f1_score: 0.9172 - val_precision: 0.9195 - val_recall: 0.9153\n",
      "Epoch 201/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0013 - acc: 0.9991 - f1_score: 0.9991 - precision: 0.9991 - recall: 0.9991 - val_loss: 0.7339 - val_acc: 0.9068 - val_f1_score: 0.9045 - val_precision: 0.9068 - val_recall: 0.9025\n",
      "Epoch 202/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0124 - acc: 0.9972 - f1_score: 0.9969 - precision: 0.9972 - recall: 0.9967 - val_loss: 0.6754 - val_acc: 0.9068 - val_f1_score: 0.9088 - val_precision: 0.9110 - val_recall: 0.9068\n",
      "Epoch 203/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0091 - acc: 0.9972 - f1_score: 0.9974 - precision: 0.9976 - recall: 0.9972 - val_loss: 0.7588 - val_acc: 0.9110 - val_f1_score: 0.9130 - val_precision: 0.9153 - val_recall: 0.9110\n",
      "Epoch 204/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0019 - acc: 0.9995 - f1_score: 0.9995 - precision: 0.9995 - recall: 0.9995 - val_loss: 0.7680 - val_acc: 0.9068 - val_f1_score: 0.9088 - val_precision: 0.9110 - val_recall: 0.9068\n",
      "Epoch 205/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0059 - acc: 0.9981 - f1_score: 0.9983 - precision: 0.9986 - recall: 0.9981 - val_loss: 0.8432 - val_acc: 0.8814 - val_f1_score: 0.8854 - val_precision: 0.8951 - val_recall: 0.8771\n",
      "Epoch 206/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0076 - acc: 0.9967 - f1_score: 0.9969 - precision: 0.9972 - recall: 0.9967 - val_loss: 0.9135 - val_acc: 0.8898 - val_f1_score: 0.8873 - val_precision: 0.8892 - val_recall: 0.8856\n",
      "Epoch 207/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0064 - acc: 0.9972 - f1_score: 0.9974 - precision: 0.9976 - recall: 0.9972 - val_loss: 0.7535 - val_acc: 0.8983 - val_f1_score: 0.9003 - val_precision: 0.9025 - val_recall: 0.8983\n",
      "Epoch 208/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0115 - acc: 0.9967 - f1_score: 0.9966 - precision: 0.9970 - recall: 0.9962 - val_loss: 0.7321 - val_acc: 0.8983 - val_f1_score: 0.8983 - val_precision: 0.8983 - val_recall: 0.8983\n",
      "Epoch 209/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0018 - acc: 0.9986 - f1_score: 0.9986 - precision: 0.9986 - recall: 0.9986 - val_loss: 0.7614 - val_acc: 0.9068 - val_f1_score: 0.9068 - val_precision: 0.9068 - val_recall: 0.9068\n",
      "Epoch 210/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0114 - acc: 0.9958 - f1_score: 0.9962 - precision: 0.9967 - recall: 0.9958 - val_loss: 0.7703 - val_acc: 0.9068 - val_f1_score: 0.9059 - val_precision: 0.9098 - val_recall: 0.9025\n",
      "Epoch 211/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0123 - acc: 0.9967 - f1_score: 0.9973 - precision: 0.9986 - recall: 0.9967 - val_loss: 0.6598 - val_acc: 0.9025 - val_f1_score: 0.9025 - val_precision: 0.9025 - val_recall: 0.9025\n",
      "Epoch 212/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0444 - acc: 0.9892 - f1_score: 0.9891 - precision: 0.9896 - recall: 0.9887 - val_loss: 0.6605 - val_acc: 0.8983 - val_f1_score: 0.8972 - val_precision: 0.9056 - val_recall: 0.8898\n",
      "Epoch 213/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0306 - acc: 0.9925 - f1_score: 0.9929 - precision: 0.9939 - recall: 0.9920 - val_loss: 0.8693 - val_acc: 0.8686 - val_f1_score: 0.8661 - val_precision: 0.8680 - val_recall: 0.8644\n",
      "Epoch 214/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0085 - acc: 0.9962 - f1_score: 0.9965 - precision: 0.9967 - recall: 0.9962 - val_loss: 0.7486 - val_acc: 0.8941 - val_f1_score: 0.8958 - val_precision: 0.8977 - val_recall: 0.8941\n",
      "Epoch 215/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0085 - acc: 0.9972 - f1_score: 0.9974 - precision: 0.9976 - recall: 0.9972 - val_loss: 0.7407 - val_acc: 0.8941 - val_f1_score: 0.8941 - val_precision: 0.8941 - val_recall: 0.8941\n",
      "Epoch 216/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0077 - acc: 0.9962 - f1_score: 0.9965 - precision: 0.9967 - recall: 0.9962 - val_loss: 0.8183 - val_acc: 0.8983 - val_f1_score: 0.8997 - val_precision: 0.9013 - val_recall: 0.8983\n",
      "Epoch 217/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0034 - acc: 0.9981 - f1_score: 0.9981 - precision: 0.9981 - recall: 0.9981 - val_loss: 0.6550 - val_acc: 0.8983 - val_f1_score: 0.8983 - val_precision: 0.8983 - val_recall: 0.8983\n",
      "Epoch 218/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0011 - acc: 0.9995 - f1_score: 0.9995 - precision: 0.9995 - recall: 0.9995 - val_loss: 0.6695 - val_acc: 0.8983 - val_f1_score: 0.8983 - val_precision: 0.8983 - val_recall: 0.8983\n",
      "Epoch 219/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0013 - acc: 0.9991 - f1_score: 0.9993 - precision: 0.9995 - recall: 0.9991 - val_loss: 0.6840 - val_acc: 0.8983 - val_f1_score: 0.9000 - val_precision: 0.9019 - val_recall: 0.8983\n",
      "Epoch 220/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0071 - acc: 0.9976 - f1_score: 0.9979 - precision: 0.9981 - recall: 0.9976 - val_loss: 0.7460 - val_acc: 0.8941 - val_f1_score: 0.8941 - val_precision: 0.8941 - val_recall: 0.8941\n",
      "Epoch 221/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0025 - acc: 0.9986 - f1_score: 0.9986 - precision: 0.9986 - recall: 0.9986 - val_loss: 0.7115 - val_acc: 0.9025 - val_f1_score: 0.9025 - val_precision: 0.9025 - val_recall: 0.9025\n",
      "Epoch 222/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0068 - acc: 0.9981 - f1_score: 0.9981 - precision: 0.9981 - recall: 0.9981 - val_loss: 0.6750 - val_acc: 0.9068 - val_f1_score: 0.9042 - val_precision: 0.9062 - val_recall: 0.9025\n",
      "Epoch 223/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0020 - acc: 0.9991 - f1_score: 0.9991 - precision: 0.9991 - recall: 0.9991 - val_loss: 0.6600 - val_acc: 0.9068 - val_f1_score: 0.9068 - val_precision: 0.9068 - val_recall: 0.9068\n",
      "Epoch 224/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0014 - acc: 0.9991 - f1_score: 0.9993 - precision: 0.9995 - recall: 0.9991 - val_loss: 0.6790 - val_acc: 0.9110 - val_f1_score: 0.9085 - val_precision: 0.9104 - val_recall: 0.9068\n",
      "Epoch 225/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0052 - acc: 0.9972 - f1_score: 0.9974 - precision: 0.9976 - recall: 0.9972 - val_loss: 0.7566 - val_acc: 0.9025 - val_f1_score: 0.9025 - val_precision: 0.9025 - val_recall: 0.9025\n",
      "Epoch 226/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0135 - acc: 0.9962 - f1_score: 0.9962 - precision: 0.9962 - recall: 0.9962 - val_loss: 0.7215 - val_acc: 0.9195 - val_f1_score: 0.9195 - val_precision: 0.9195 - val_recall: 0.9195\n",
      "Epoch 227/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0037 - acc: 0.9981 - f1_score: 0.9981 - precision: 0.9981 - recall: 0.9981 - val_loss: 0.5973 - val_acc: 0.9280 - val_f1_score: 0.9232 - val_precision: 0.9274 - val_recall: 0.9195\n",
      "Epoch 228/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0118 - acc: 0.9958 - f1_score: 0.9959 - precision: 0.9966 - recall: 0.9953 - val_loss: 0.7716 - val_acc: 0.9025 - val_f1_score: 0.9025 - val_precision: 0.9025 - val_recall: 0.9025\n",
      "Epoch 229/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0058 - acc: 0.9981 - f1_score: 0.9983 - precision: 0.9985 - recall: 0.9981 - val_loss: 0.6757 - val_acc: 0.9025 - val_f1_score: 0.9017 - val_precision: 0.9056 - val_recall: 0.8983\n",
      "Epoch 230/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0034 - acc: 0.9986 - f1_score: 0.9986 - precision: 0.9986 - recall: 0.9986 - val_loss: 0.7624 - val_acc: 0.8941 - val_f1_score: 0.8915 - val_precision: 0.8935 - val_recall: 0.8898\n",
      "Epoch 231/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0021 - acc: 0.9995 - f1_score: 0.9993 - precision: 0.9995 - recall: 0.9991 - val_loss: 0.6864 - val_acc: 0.9025 - val_f1_score: 0.9025 - val_precision: 0.9025 - val_recall: 0.9025\n",
      "Epoch 232/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0051 - acc: 0.9972 - f1_score: 0.9974 - precision: 0.9976 - recall: 0.9972 - val_loss: 0.7452 - val_acc: 0.9110 - val_f1_score: 0.9110 - val_precision: 0.9110 - val_recall: 0.9110\n",
      "Epoch 233/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0047 - acc: 0.9981 - f1_score: 0.9983 - precision: 0.9986 - recall: 0.9981 - val_loss: 0.7576 - val_acc: 0.9153 - val_f1_score: 0.9153 - val_precision: 0.9153 - val_recall: 0.9153\n",
      "Epoch 234/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0037 - acc: 0.9986 - f1_score: 0.9986 - precision: 0.9986 - recall: 0.9986 - val_loss: 0.7419 - val_acc: 0.8983 - val_f1_score: 0.8960 - val_precision: 0.8983 - val_recall: 0.8941\n",
      "Epoch 235/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0012 - acc: 0.9995 - f1_score: 0.9995 - precision: 0.9995 - recall: 0.9995 - val_loss: 0.7417 - val_acc: 0.9025 - val_f1_score: 0.9025 - val_precision: 0.9025 - val_recall: 0.9025\n",
      "Epoch 236/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0041 - acc: 0.9976 - f1_score: 0.9981 - precision: 0.9986 - recall: 0.9976 - val_loss: 0.8226 - val_acc: 0.8941 - val_f1_score: 0.8977 - val_precision: 0.9019 - val_recall: 0.8941\n",
      "Epoch 237/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0027 - acc: 0.9986 - f1_score: 0.9986 - precision: 0.9986 - recall: 0.9986 - val_loss: 0.8054 - val_acc: 0.8941 - val_f1_score: 0.8960 - val_precision: 0.8983 - val_recall: 0.8941\n",
      "Epoch 238/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0021 - acc: 0.9991 - f1_score: 0.9991 - precision: 0.9991 - recall: 0.9991 - val_loss: 0.7069 - val_acc: 0.9068 - val_f1_score: 0.9088 - val_precision: 0.9110 - val_recall: 0.9068\n",
      "Epoch 239/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0022 - acc: 0.9991 - f1_score: 0.9991 - precision: 0.9991 - recall: 0.9991 - val_loss: 0.6578 - val_acc: 0.9110 - val_f1_score: 0.9147 - val_precision: 0.9189 - val_recall: 0.9110\n",
      "Epoch 240/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0020 - acc: 0.9991 - f1_score: 0.9991 - precision: 0.9991 - recall: 0.9991 - val_loss: 0.6439 - val_acc: 0.9110 - val_f1_score: 0.9130 - val_precision: 0.9153 - val_recall: 0.9110\n",
      "Epoch 241/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0160 - acc: 0.9958 - f1_score: 0.9958 - precision: 0.9958 - recall: 0.9958 - val_loss: 0.8113 - val_acc: 0.8941 - val_f1_score: 0.8956 - val_precision: 0.9025 - val_recall: 0.8898\n",
      "Epoch 242/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0024 - acc: 0.9986 - f1_score: 0.9986 - precision: 0.9986 - recall: 0.9986 - val_loss: 0.7705 - val_acc: 0.9110 - val_f1_score: 0.9110 - val_precision: 0.9110 - val_recall: 0.9110\n",
      "Epoch 243/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0018 - acc: 0.9995 - f1_score: 0.9993 - precision: 0.9995 - recall: 0.9991 - val_loss: 0.7504 - val_acc: 0.9153 - val_f1_score: 0.9171 - val_precision: 0.9195 - val_recall: 0.9153\n",
      "Epoch 244/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 8.7494e-04 - acc: 0.9995 - f1_score: 0.9995 - precision: 0.9995 - recall: 0.9995 - val_loss: 0.7501 - val_acc: 0.9153 - val_f1_score: 0.9171 - val_precision: 0.9195 - val_recall: 0.9153\n",
      "Epoch 245/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0017 - acc: 0.9991 - f1_score: 0.9991 - precision: 0.9991 - recall: 0.9991 - val_loss: 0.7468 - val_acc: 0.9110 - val_f1_score: 0.9128 - val_precision: 0.9153 - val_recall: 0.9110\n",
      "Epoch 246/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0078 - acc: 0.9976 - f1_score: 0.9976 - precision: 0.9976 - recall: 0.9976 - val_loss: 0.7255 - val_acc: 0.8814 - val_f1_score: 0.8839 - val_precision: 0.8916 - val_recall: 0.8771\n",
      "Epoch 247/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0092 - acc: 0.9972 - f1_score: 0.9974 - precision: 0.9976 - recall: 0.9972 - val_loss: 0.7877 - val_acc: 0.8898 - val_f1_score: 0.8915 - val_precision: 0.8935 - val_recall: 0.8898\n",
      "Epoch 248/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0089 - acc: 0.9976 - f1_score: 0.9976 - precision: 0.9976 - recall: 0.9976 - val_loss: 0.6962 - val_acc: 0.9025 - val_f1_score: 0.9062 - val_precision: 0.9104 - val_recall: 0.9025\n",
      "Epoch 249/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0188 - acc: 0.9967 - f1_score: 0.9962 - precision: 0.9967 - recall: 0.9958 - val_loss: 0.7941 - val_acc: 0.8941 - val_f1_score: 0.8900 - val_precision: 0.9011 - val_recall: 0.8814\n",
      "Epoch 250/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0071 - acc: 0.9976 - f1_score: 0.9976 - precision: 0.9976 - recall: 0.9976 - val_loss: 0.8070 - val_acc: 0.8983 - val_f1_score: 0.9020 - val_precision: 0.9062 - val_recall: 0.8983\n",
      "Epoch 251/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0026 - acc: 0.9991 - f1_score: 0.9988 - precision: 0.9990 - recall: 0.9986 - val_loss: 0.8050 - val_acc: 0.9068 - val_f1_score: 0.9037 - val_precision: 0.9098 - val_recall: 0.8983\n",
      "Epoch 252/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0137 - acc: 0.9953 - f1_score: 0.9950 - precision: 0.9953 - recall: 0.9948 - val_loss: 0.7655 - val_acc: 0.8814 - val_f1_score: 0.8802 - val_precision: 0.8838 - val_recall: 0.8771\n",
      "Epoch 253/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0078 - acc: 0.9976 - f1_score: 0.9973 - precision: 0.9980 - recall: 0.9967 - val_loss: 0.7829 - val_acc: 0.8856 - val_f1_score: 0.8873 - val_precision: 0.8892 - val_recall: 0.8856\n",
      "Epoch 254/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0202 - acc: 0.9948 - f1_score: 0.9948 - precision: 0.9953 - recall: 0.9943 - val_loss: 0.8905 - val_acc: 0.8771 - val_f1_score: 0.8788 - val_precision: 0.8808 - val_recall: 0.8771\n",
      "Epoch 255/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0188 - acc: 0.9948 - f1_score: 0.9950 - precision: 0.9952 - recall: 0.9948 - val_loss: 0.6943 - val_acc: 0.8983 - val_f1_score: 0.8969 - val_precision: 0.9050 - val_recall: 0.8898\n",
      "Epoch 256/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0064 - acc: 0.9962 - f1_score: 0.9962 - precision: 0.9962 - recall: 0.9962 - val_loss: 0.7552 - val_acc: 0.8983 - val_f1_score: 0.9003 - val_precision: 0.9025 - val_recall: 0.8983\n",
      "Epoch 257/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0052 - acc: 0.9981 - f1_score: 0.9983 - precision: 0.9986 - recall: 0.9981 - val_loss: 0.6971 - val_acc: 0.9025 - val_f1_score: 0.9045 - val_precision: 0.9068 - val_recall: 0.9025\n",
      "Epoch 258/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0044 - acc: 0.9986 - f1_score: 0.9988 - precision: 0.9991 - recall: 0.9986 - val_loss: 0.6272 - val_acc: 0.9110 - val_f1_score: 0.9086 - val_precision: 0.9110 - val_recall: 0.9068\n",
      "Epoch 259/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0042 - acc: 0.9981 - f1_score: 0.9981 - precision: 0.9981 - recall: 0.9981 - val_loss: 0.7185 - val_acc: 0.9110 - val_f1_score: 0.9065 - val_precision: 0.9110 - val_recall: 0.9025\n",
      "Epoch 260/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0212 - acc: 0.9948 - f1_score: 0.9949 - precision: 0.9951 - recall: 0.9948 - val_loss: 0.7133 - val_acc: 0.8898 - val_f1_score: 0.8907 - val_precision: 0.8965 - val_recall: 0.8856\n",
      "Epoch 261/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0480 - acc: 0.9887 - f1_score: 0.9886 - precision: 0.9895 - recall: 0.9878 - val_loss: 1.0587 - val_acc: 0.8602 - val_f1_score: 0.8540 - val_precision: 0.8638 - val_recall: 0.8475\n",
      "Epoch 262/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0208 - acc: 0.9948 - f1_score: 0.9946 - precision: 0.9948 - recall: 0.9943 - val_loss: 0.6055 - val_acc: 0.9153 - val_f1_score: 0.9172 - val_precision: 0.9195 - val_recall: 0.9153\n",
      "Epoch 263/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0059 - acc: 0.9967 - f1_score: 0.9967 - precision: 0.9967 - recall: 0.9967 - val_loss: 0.5673 - val_acc: 0.9237 - val_f1_score: 0.9254 - val_precision: 0.9274 - val_recall: 0.9237\n",
      "Epoch 264/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0033 - acc: 0.9986 - f1_score: 0.9988 - precision: 0.9995 - recall: 0.9981 - val_loss: 0.6011 - val_acc: 0.9153 - val_f1_score: 0.9169 - val_precision: 0.9189 - val_recall: 0.9153\n",
      "Epoch 265/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0045 - acc: 0.9986 - f1_score: 0.9986 - precision: 0.9986 - recall: 0.9986 - val_loss: 0.5940 - val_acc: 0.9153 - val_f1_score: 0.9169 - val_precision: 0.9189 - val_recall: 0.9153\n",
      "Epoch 266/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0051 - acc: 0.9986 - f1_score: 0.9986 - precision: 0.9986 - recall: 0.9986 - val_loss: 0.6330 - val_acc: 0.9068 - val_f1_score: 0.9085 - val_precision: 0.9104 - val_recall: 0.9068\n",
      "Epoch 267/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0061 - acc: 0.9981 - f1_score: 0.9983 - precision: 0.9986 - recall: 0.9981 - val_loss: 0.6219 - val_acc: 0.9068 - val_f1_score: 0.9085 - val_precision: 0.9104 - val_recall: 0.9068\n",
      "Epoch 268/1000\n",
      "2123/2123 [==============================] - 11s 5ms/step - loss: 0.0020 - acc: 0.9995 - f1_score: 0.9992 - precision: 0.9995 - recall: 0.9991 - val_loss: 0.6362 - val_acc: 0.9068 - val_f1_score: 0.9085 - val_precision: 0.9104 - val_recall: 0.9068\n",
      "Epoch 269/1000\n",
      "  56/2123 [..............................] - ETA: 10s - loss: 3.6784e-04 - acc: 1.0000 - f1_score: 1.0000 - precision: 1.0000 - recall: 1.0000"
     ]
    }
   ],
   "source": [
    "index= 0\n",
    "step = 1000\n",
    "stop = 1000\n",
    "\n",
    "while index < stop:\n",
    "    hist = model.fit(x_train, y_train, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_val, y_val), callbacks=[tbCallBack])\n",
    "    index += step\n",
    "    history.append(hist)\n",
    "    model.save_weights('data/models/model_coarse'+str(index)+'.h5')\n",
    "save_index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_histo_vis(history, \"history/Single_cls\")\n",
    "save_his(history, \"history/single_cls.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Most Recent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_fine = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get weights and biases of all layers\n",
    "#for layer in model.layers: print(layer.get_config(), layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will freeze all layers\n",
    "for i in range(len(model.layers)):\n",
    "    model.layers[i].trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning for Coarse Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_c = np.dot(y_train,fine2coarse)\n",
    "y_val_c = np.dot(y_val,fine2coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GGN:\n",
    "    # Inception\n",
    "    net = Conv2D(1024, 1, strides=1, padding='same', activation='relu')(model.layers[-8].output)\n",
    "    net = Conv2D(1152, 2, strides=1, padding='same', activation='relu')(net)\n",
    "else:\n",
    "    # NiN\n",
    "    net = Conv2D(1024, 1, strides=1, padding='same', activation='relu')(model.layers[-8].output)\n",
    "    net = Conv2D(1152, 2, strides=1, padding='same', activation='relu')(net)\n",
    "net = Dropout(.6)(net)\n",
    "net = MaxPooling2D((2, 2), padding='same')(net)\n",
    "\n",
    "net = Flatten()(net)\n",
    "net = Dense(1152, activation='relu')(net)\n",
    "out_coarse = Dense(20, activation='softmax')(net)\n",
    "\n",
    "model_c = Model(inputs=in_layer,outputs=out_coarse)\n",
    "model_c.compile(optimizer= sgd_coarse, loss='categorical_crossentropy', metrics=['accuracy', f1_score, precision, recall])\n",
    "\n",
    "# model.load_weights('data/models/model_coarse'+str(10)+'.h5')\n",
    "\n",
    "# for i in range(len(model_c.layers)-1):\n",
    "#     model_c.layers[i].set_weights(model.layers[i].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('data/models/model_coarse'+str(10)+'.h5')\n",
    "\n",
    "for i in range(len(model_c.layers)-1):\n",
    "    model_c.layers[i].set_weights(model.layers[i].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1000\n",
    "step = 1000\n",
    "stop = 2000\n",
    "\n",
    "history = []\n",
    "while index < stop:\n",
    "    hist = model_c.fit(x_train, y_train_c, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_val, y_val_c), callbacks=[tbCallBack])\n",
    "    index += step\n",
    "    history.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_histo_vis(history, \"history/Coarse_cls1\")\n",
    "save_his(history, \"history/coarse_cls1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.compile(optimizer=sgd_fine, loss='categorical_crossentropy', metrics=['accuracy', f1_score, precision, recall])\n",
    "stop = 3000\n",
    "\n",
    "history = []\n",
    "while index < stop:\n",
    "    hist = model_c.fit(x_train, y_train_c, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_val, y_val_c), callbacks=[tbCallBack])\n",
    "    index += step\n",
    "    history.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_histo_vis(history, \"history/Coarse_cls2\")\n",
    "save_his(history, \"history/coarse_cls2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning for Fine Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Fine Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_model():\n",
    "    net = Conv2D(1024, 1, strides=1, padding='same', activation='relu')(model.layers[-8].output)\n",
    "    net = Conv2D(1152, 2, strides=1, padding='same', activation='relu')(net)\n",
    "    net = Dropout(.2)(net)\n",
    "    net = MaxPooling2D((2, 2), padding='same')(net)\n",
    "\n",
    "    net = Flatten()(net)\n",
    "    net = Dense(1152, activation='elu')(net)\n",
    "    out_fine = Dense(61, activation='softmax')(net)\n",
    "    model_fine = Model(inputs=in_layer,outputs=out_fine)\n",
    "    model_fine.compile(optimizer= sgd_coarse,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', f1_score, precision, recall])\n",
    "    \n",
    "    for i in range(len(model_fine.layers)-1):\n",
    "        model_fine.layers[i].set_weights(model.layers[i].get_weights())\n",
    "    return model_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_models = {'models' : [{} for i in range(coarse_categories)], 'yhf' : [{} for i in range(coarse_categories)]}\n",
    "for i in range(coarse_categories):\n",
    "    model_i = fine_model()\n",
    "    fine_models['models'][i] = model_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Fine Classifiers on Respective Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(y,yh):\n",
    "    # Threshold \n",
    "    yht = np.zeros(np.shape(yh))\n",
    "    yht[np.arange(len(yh)), yh.argmax(1)] = 1\n",
    "    # Evaluate Error\n",
    "    error = float(np.count_nonzero(np.count_nonzero(y-yht,1)))/len(y)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(coarse_categories):\n",
    "    index= 0\n",
    "    step = 500\n",
    "    stop = 500\n",
    "    \n",
    "    # Get all training data for the coarse category\n",
    "    ix = np.where([(y_train[:,j]==1) for j in [k for k, e in enumerate(fine2coarse[:,i]) if e != 0]])[1]\n",
    "    x_tix = x_train[ix]\n",
    "    y_tix = y_train[ix]\n",
    "    \n",
    "    # Get all validation data for the coarse category\n",
    "    ix_v = np.where([(y_val[:,j]==1) for j in [k for k, e in enumerate(fine2coarse[:,i]) if e != 0]])[1]\n",
    "    x_vix = x_val[ix_v]\n",
    "    y_vix = y_val[ix_v]\n",
    "    \n",
    "    history = []\n",
    "    while index < stop:\n",
    "        hist = fine_models['models'][i].fit(x_tix, y_tix, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_vix, y_vix))\n",
    "        index += step\n",
    "        history.append(hist)\n",
    "    train_histo_vis(history, \"history/Fine_cls1\")\n",
    "    save_his(history, \"history/fine_cls1.json\")\n",
    "    \n",
    "    fine_models['models'][i].compile(optimizer=sgd_fine, loss='categorical_crossentropy', metrics=['accuracy', f1_score, precision, recall])\n",
    "    stop = 1000\n",
    "    \n",
    "    history = []\n",
    "    while index < stop:\n",
    "        hist = fine_models['models'][i].fit(x_tix, y_tix, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_vix, y_vix))\n",
    "        index += step\n",
    "        history.append(hist)\n",
    "    train_histo_vis(history, \"history/Fine_cls2\")\n",
    "    save_his(history, \"history/fine_cls2.json\")\n",
    "        \n",
    "    yh_f = fine_models['models'][i].predict(x_val[ix_v], batch_size=batch)\n",
    "    print('Fine Classifier '+str(i)+' Error: '+str(get_error(y_val[ix_v],yh_f))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def eval_hdcnn(X, y):\n",
    "    yh = np.zeros(np.shape(y))\n",
    "    \n",
    "    yh_s = model.predict(X, batch_size=batch)\n",
    "    \n",
    "    print('Single Classifier Error: '+str(get_error(y,yh_s)))\n",
    "    \n",
    "    yh_c = model_c.predict(X, batch_size=batch)\n",
    "    y_c = np.dot(y,fine2coarse)\n",
    "    \n",
    "    print('Coarse Classifier Error: '+str(get_error(y_c,yh_c)))\n",
    "\n",
    "    for i in range(coarse_categories):\n",
    "        if i%5 == 0:\n",
    "            print(\"Evaluating Fine Classifier: \", str(i))\n",
    "        #fine_models['yhf'][i] = fine_models['models'][i].predict(X, batch_size=batch)\n",
    "        yh += np.multiply(yh_c[:,i].reshape((len(y)),1), fine_models['yhf'][i])\n",
    "    \n",
    "    print('Overall Error: '+str(get_error(y,yh)))\n",
    "    return yh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yh = eval_hdcnn(x_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_test; y = y_test\n",
    "\n",
    "yh = np.zeros(np.shape(y))\n",
    "\n",
    "yh_s = model.predict(X, batch_size=batch)\n",
    "\n",
    "print('Single Classifier Error: '+str(get_error(y,yh_s)))\n",
    "\n",
    "yh_c = model_c.predict(X, batch_size=batch)\n",
    "y_c = np.dot(y,fine2coarse)\n",
    "\n",
    "print('Coarse Classifier Error: '+str(get_error(y_c,yh_c)))\n",
    "\n",
    "for i in range(coarse_categories):\n",
    "    if i%5 == 0:\n",
    "        print(\"Evaluating Fine Classifier: \"+ str(i))\n",
    "    fine_models['yhf'][i] = fine_models['models'][i].predict(X, batch_size=batch)\n",
    "    yh += np.multiply(yh_c[:,i].reshape((len(y)),1), fine_models['yhf'][i])\n",
    "\n",
    "print('Overall Error: '+str(get_error(y,yh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get error function: Debug\n",
    "yht = np.zeros(np.shape(yh))\n",
    "yht[np.arange(len(yh)), yh.argmax(1)] = 1\n",
    "# Evaluate Error\n",
    "error = float(np.count_nonzero(np.count_nonzero(y-yht,1)))/len(y)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
